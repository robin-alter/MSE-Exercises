{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 01:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "\n",
    "digits = load_digits()\n",
    "data = digits[\"data\"]\n",
    "images = digits[\"images\"]\n",
    "target = digits[\"target\"]\n",
    "target_names = digits[\"target_names\"]\n",
    "\n",
    "print(np.shape(images[0]))\n",
    "#The images are of size 8x8\n",
    "\n",
    "#Interpolate using nearest neighbor\n",
    "img_nearest = images[3]\n",
    "assert 2 == len (img_nearest.shape)\n",
    "plt.figure()\n",
    "plt.gray()\n",
    "plt.imshow(img_nearest,interpolation = \"nearest\" ) # also try interpolation =\" bicubic \"\n",
    "plt.show()\n",
    "\n",
    "#Interpolate using bicubic\n",
    "img_bicubic = images[3]\n",
    "assert 2 == len (img_bicubic.shape)\n",
    "plt.figure()\n",
    "plt.gray()\n",
    "plt.imshow(img_bicubic,interpolation = \"bicubic\" ) # also try interpolation =\" bicubic \"\n",
    "plt.show()\n",
    "\n",
    "x_3and9 = []\n",
    "y_3and9 = []\n",
    "image_3and9 = []\n",
    "for i in range(len(data)):\n",
    "    if target[i] == 3 or target[i] == 9:\n",
    "        x_3and9.append(data[i])\n",
    "        y_3and9.append(target[i]) \n",
    "        image_3and9.append(images[i])\n",
    "\n",
    "X_train, X_test, y_train, y_test =\\\n",
    "    model_selection.train_test_split(x_3and9, y_3and9, test_size = 0.4, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2.1\n",
    "\n",
    "# Calculate average images for digits 3 and 9\n",
    "avg_digit_3 = np.mean(data[target == 3], axis=0).reshape(8, 8)\n",
    "avg_digit_9 = np.mean(data[target == 9], axis=0).reshape(8, 8)\n",
    "difference = avg_digit_3 - avg_digit_9\n",
    "\n",
    "# Visualize the average images and their differences\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(avg_digit_3, cmap='gray', interpolation='none')\n",
    "plt.title('Average 3')\n",
    "plt.colorbar()\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(avg_digit_9, cmap='gray', interpolation='none')\n",
    "plt.title('Average 9')\n",
    "plt.colorbar()\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(difference, cmap='gray', interpolation='none')\n",
    "plt.title('Difference (3-9)')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# Identify the indices of the two pixels with the largest absolute differences\n",
    "indices = np.unravel_index(np.argsort(-np.abs(difference).ravel())[:2], difference.shape)\n",
    "pixel_1, pixel_2 = indices\n",
    "\n",
    "# Print pixel coordinates in the (row, column) format\n",
    "print(f\"Selected Pixel 1: Row {pixel_1[0]}, Column {pixel_1[1]} (0-indexed)\")\n",
    "print(f\"Selected Pixel 2: Row {pixel_2[0]}, Column {pixel_2[1]} (0-indexed)\")\n",
    "\n",
    "# Function to extract 2D features from the dataset\n",
    "def features2d(x):\n",
    "    \"\"\" Extract two features based on predefined pixel indices. \"\"\"\n",
    "    f1 = x[:, pixel_1[0] * 8 + pixel_1[1]]  # Convert 2D index to 1D index for Feature 1\n",
    "    f2 = x[:, pixel_2[0] * 8 + pixel_2[1]]  # Convert 2D index to 1D index for Feature 2\n",
    "    return np.column_stack((f1, f2))\n",
    "\n",
    "# Extract 2D features for the training data\n",
    "X_train_2d = features2d(np.array(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 2.2\n",
    "\n",
    "# Convert training labels to a NumPy array for easier indexing\n",
    "y_train_array = np.array(y_train)\n",
    "\n",
    "# Set up the figure for scatter plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Pre-select data for each class to avoid repetitive indexing\n",
    "class_3_data = X_train_2d[y_train_array == 3]\n",
    "class_9_data = X_train_2d[y_train_array == 9]\n",
    "\n",
    "# Scatter plot for class '3'\n",
    "plt.scatter(class_3_data[:, 0], class_3_data[:, 1], \n",
    "            color='darkred', marker='o', edgecolor='black', s=60, alpha=0.8, label='Digit 3')\n",
    "\n",
    "# Scatter plot for class '9'\n",
    "plt.scatter(class_9_data[:, 0], class_9_data[:, 1], \n",
    "            color='navy', marker='^', edgecolor='white', s=60, alpha=0.8, label='Digit 9')\n",
    "\n",
    "# Dynamic axis labels based on pixel indices\n",
    "plt.xlabel(f'Feature 1 (Pixel at ({pixel_1[0]}, {pixel_1[1]}))', fontsize=14)\n",
    "plt.ylabel(f'Feature 2 (Pixel at ({pixel_2[0]}, {pixel_2[1]}))', fontsize=14)\n",
    "\n",
    "# Set a descriptive title for the plot\n",
    "plt.title('Feature Space for Digits 3 and 9', fontsize=16)\n",
    "\n",
    "# Add a legend to help identify the classes\n",
    "plt.legend(title='Classes', title_fontsize='13', fontsize='12', loc='upper right')\n",
    "\n",
    "# Enable grid for better alignment\n",
    "plt.grid(True)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.3\n",
    "\n",
    "# Convert test data to a NumPy array and extract 2D features\n",
    "X_test_array = np.array(X_test)\n",
    "X_test_2d = features2d(X_test_array)\n",
    "\n",
    "# Function to adjust labels to -1 for '3' and 1 for '9'\n",
    "def adjust_labels(labels):\n",
    "    return np.where(labels == 3, -1, 1)\n",
    "\n",
    "# Adjust labels for training and test datasets\n",
    "y_train_labels = adjust_labels(np.array(y_train))  # Ensure y_train is converted to a NumPy array if not already\n",
    "y_test_labels = adjust_labels(np.array(y_test))    # Convert y_test to a NumPy array for consistent processing\n",
    "\n",
    "# Define the nearest mean classifier optimized with vectorized operations\n",
    "def nearest_mean_classifier(train_features, train_labels, test_features):\n",
    "    mean_digit_3 = np.mean(train_features[train_labels == -1], axis=0)\n",
    "    mean_digit_9 = np.mean(train_features[train_labels == 1], axis=0)\n",
    "    \n",
    "    # Calculate distances to each mean for all test features at once\n",
    "    dist_to_3 = np.linalg.norm(test_features - mean_digit_3, axis=1)\n",
    "    dist_to_9 = np.linalg.norm(test_features - mean_digit_9, axis=1)\n",
    "    predicted_labels = np.where(dist_to_3 < dist_to_9, -1, 1)\n",
    "    \n",
    "    return predicted_labels\n",
    "\n",
    "# Classify the training and test datasets\n",
    "predicted_train_labels = nearest_mean_classifier(X_train_2d, y_train_labels, X_train_2d)\n",
    "predicted_test_labels = nearest_mean_classifier(X_train_2d, y_train_labels, X_test_2d)\n",
    "\n",
    "# Function to calculate classification error\n",
    "def classification_error(true_labels, predicted_labels):\n",
    "    return np.mean(true_labels != predicted_labels)\n",
    "\n",
    "# Calculate and print the classification errors\n",
    "train_error = classification_error(y_train_labels, predicted_train_labels)\n",
    "test_error = classification_error(y_test_labels, predicted_test_labels)\n",
    "print(f\"Training Error: {train_error:.2f}\")\n",
    "print(f\"Testing Error: {test_error:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 2.4\n",
    "\n",
    "# Create a grid of points in the feature space\n",
    "x1_min, x1_max = X_train_2d[:, 0].min(), X_train_2d[:, 0].max()\n",
    "x2_min, x2_max = X_train_2d[:, 1].min(), X_train_2d[:, 1].max()\n",
    "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 200), np.linspace(x2_min, x2_max, 200))\n",
    "grid_points = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "\n",
    "# Predict class for each point on the grid using the correct function and variables\n",
    "predicted_labels = nearest_mean_classifier(X_train_2d, y_train_labels, grid_points).reshape(xx1.shape)\n",
    "\n",
    "# Plot the decision regions\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contourf(xx1, xx2, predicted_labels, alpha=0.5, cmap='RdBu', levels=np.array([-1, 0, 1]))\n",
    "\n",
    "# Plot class means\n",
    "mean_digit_3 = np.mean(X_train_2d[y_train_labels == -1], axis=0)\n",
    "mean_digit_9 = np.mean(X_train_2d[y_train_labels == 1], axis=0)\n",
    "plt.scatter(mean_digit_3[0], mean_digit_3[1], c='black', marker='o', s=100, label='Mean of Digit 3')\n",
    "plt.scatter(mean_digit_9[0], mean_digit_9[1], c='black', marker='s', s=100, label='Mean of Digit 9')\n",
    "\n",
    "# Plot test data points\n",
    "# Ensure test data points are colored according to their actual labels\n",
    "colors = ['red' if label == -1 else 'blue' for label in y_test_labels]\n",
    "plt.scatter(X_test_2d[:, 0], X_test_2d[:, 1], color=colors, edgecolors='k', label='Test Data')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('Feature 1 (Pixel Intensity at selected Pixel 1)')\n",
    "plt.ylabel('Feature 2 (Pixel Intensity at selected Pixel 2)')\n",
    "plt.title('Decision Regions and Test Data Visualization')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 3.1\n",
    "\n",
    "def fit_lda(training_features, training_labels):\n",
    "    m_1 = np.array([0,0])\n",
    "    m1 = np.array([0,0])\n",
    "    covmat_1 = np.matrix([[0,0], [0,0]])\n",
    "    covmat1 = np.matrix([[0,0], [0,0]])\n",
    "    N = len(training_features)\n",
    "    \n",
    "    ## mu-matrix\n",
    "    for i in range(N):\n",
    "        if training_labels[i] == -1:\n",
    "            m_1 = m_1 + training_features[i]\n",
    "        else:\n",
    "            m1 = m1 + training_features[i]\n",
    "    m_1 = m_1*(1/sum(training_labels == -1))\n",
    "    m1 = m1*(1/sum(training_labels == 1))\n",
    "    mu = np.matrix([m_1, m1])\n",
    "    for i in range(N):\n",
    "        if training_labels[i] == -1:\n",
    "            temp1 = np.matrix(training_features[i]-mu[0])\n",
    "            covmat_1 = covmat_1 + np.transpose(temp1).dot(temp1)\n",
    "        else:\n",
    "            temp1 = np.matrix(training_features[i]-mu[1])\n",
    "            covmat1 = covmat1 + np.transpose(temp1).dot(temp1)\n",
    "    covmat = covmat_1 + covmat1\n",
    "    covmat = covmat/N\n",
    "    p = 0\n",
    "    \n",
    "    return mu, covmat, p\n",
    "training_features = features2d(X_train)\n",
    "training_labels = correct_labels(y_train)\n",
    "mu, covmat, p = fit_lda(training_features, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 3.2\n",
    "\n",
    "import math \n",
    "def predict_lda(mu, covmat, p, test_features, test_labels):\n",
    "    N1 = sum(test_labels == 1)\n",
    "    N_1 = sum(test_labels == -1)\n",
    "    N = N1+N_1\n",
    "    predicted_labels = []\n",
    "    \n",
    "    beta = np.linalg.inv(covmat).dot(np.transpose(mu[1]-mu[0]))\n",
    "    b = -0.5*(mu[1]+mu[0]).dot(beta)+math.log10(N1/N_1)\n",
    "    \n",
    "    for i in test_features:\n",
    "        predicted_labels.append(sign(i,beta,b))\n",
    "    return np.array(predicted_labels)\n",
    "\n",
    "def sign(x,beta,b):\n",
    "    x = x.dot(beta)+b\n",
    "    if x > -b:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "training_labels = correct_labels(y_train)\n",
    "training_prediction = predict_lda(mu, covmat, p, training_features, training_labels)\n",
    "print(\"Training Error:\", np.mean(training_prediction != training_labels))\n",
    "\n",
    "test_features = features2d(X_test)\n",
    "test_labels = correct_labels(y_test)\n",
    "test_prediction = predict_lda(mu, covmat, p, test_features, test_labels)\n",
    "print(\"Test Error:\", np.mean(test_prediction != test_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 3.3\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "\n",
    "def plot_decision_regions(features, labels, mu, covmat, p):\n",
    "    # Define the grid\n",
    "    x_min, x_max = features[:, 0].min() - 1, features[:, 0].max() + 1\n",
    "    y_min, y_max = features[:, 1].min() - 1, features[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    Z = predict_lda(mu, covmat, p, grid)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.RdBu, alpha=.8)\n",
    "    plt.scatter(features[:, 0], features[:, 1], c=labels, cmap=plt.cm.RdBu, edgecolors='black')\n",
    "    \n",
    "    for i, (m, c) in enumerate(zip(mu, covmat)):\n",
    "        eigenvalues, eigenvectors = np.linalg.eig(c)\n",
    "        std_devs = np.sqrt(eigenvalues)\n",
    "        angle = np.degrees(np.arctan2(*eigenvectors[0][::-1]))\n",
    "        ellipse = Ellipse(xy=m, width=2 * std_devs[0], height=2 * std_devs[1], angle=angle, alpha=0.3)\n",
    "        plt.gca().add_artist(ellipse)\n",
    "        ellipse.set_facecolor('none')\n",
    "        if i == 0:\n",
    "            ellipse.set_edgecolor('red')\n",
    "        else:\n",
    "            ellipse.set_edgecolor('blue')\n",
    "\n",
    "        for j, std_dev in enumerate(std_devs):\n",
    "            if eigenvectors[j][0] == 0:\n",
    "                angle = np.pi / 2\n",
    "            else:\n",
    "                angle = np.arctan(eigenvectors[j][1] / eigenvectors[j][0])\n",
    "            x, y = m + std_dev * np.array([np.cos(angle), np.sin(angle)])\n",
    "            plt.plot([m[0], x], [m[1], y], '-', color='black')\n",
    "\n",
    "    plt.legend(['Class 3', 'Class 9', 'Cluster 3', 'Cluster 9'])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "mu, covmat, p = fit_lda(train_features, y_train)\n",
    "try:\n",
    "    plot_decision_regions(train_features, y_train, mu, covmat, p)\n",
    "except:\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 3.4\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "y[y == 3] = -1\n",
    "y[y == 9] = 1\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "test_errors = []\n",
    "sklearn_test_errors = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    mu, covmat, p = fit_lda(X_train, y_train)\n",
    "    y_pred = predict_lda(mu, covmat, p, X_test)\n",
    "    test_error = np.mean(y_pred != y_test)\n",
    "    test_errors.append(test_error)\n",
    "\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    lda.fit(X_train, y_train)\n",
    "    sklearn_y_pred = lda.predict(X_test)\n",
    "    sklearn_test_error = np.mean(sklearn_y_pred != y_test)\n",
    "    sklearn_test_errors.append(sklearn_test_error)\n",
    "\n",
    "print(\"Our LDA test error: \", np.mean(test_errors))\n",
    "print(\"Sklearn LDA test error: \", np.mean(sklearn_test_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 04\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "def relu(t):\n",
    "    if( t <= 0):\n",
    "        return 0\n",
    "    elif(t > 0):\n",
    "        return t\n",
    "\n",
    "# Calculate the loss for the svm approach\n",
    "def calculate_loss(training_features, training_labels, beta, b, lam):\n",
    "    n = len(training_labels)\n",
    "    sum = 0\n",
    "    for i in range(n):\n",
    "        sum += relu(1 - (training_labels[i]*(training_features[i].dot(beta) + b)))\n",
    "\n",
    "    loss = ((1/2) * (np.transpose(beta).dot(beta))) + ((lam/n) * sum)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Calculate the loss derivation by deriving for beta\n",
    "def calculate_loss_derive_beta(training_features, training_labels, beta, b, lam):\n",
    "    sum = 0\n",
    "    n = len(training_labels)\n",
    "    for i in range(n):\n",
    "        if((training_labels[i] * (training_features[i].dot(beta) + b)) < 1):\n",
    "            sum += (-training_labels[i] * np.transpose(training_features[i].reshape(1,len(training_features[i]))))\n",
    "             \n",
    "    return beta + ((lam/n)*sum)\n",
    "\n",
    "# Calculate the loss derivation by deriving for b\n",
    "def calculate_loss_derive_b(training_features, training_labels, beta, b, lam):\n",
    "    sum = 0\n",
    "    n = len(training_labels)\n",
    "\n",
    "    for i in range(n):\n",
    "        if((training_labels[i] * (training_features[i].dot(beta) + b)) < 1):\n",
    "            sum += (-training_labels[i])\n",
    "\n",
    "    return (lam/n)*sum\n",
    "\n",
    "def fit_svm(training_features, training_labels, beta, b, lam, learning_rate):\n",
    "    #Calculate loss for current beta and b\n",
    "    loss = calculate_loss(training_features, training_labels, beta, b, lam)\n",
    "    \n",
    "    #calculate new beta and b\n",
    "    new_beta = beta - learning_rate * calculate_loss_derive_beta(training_features, training_labels, beta, b, lam)\n",
    "    new_b = b - learning_rate * calculate_loss_derive_b(training_features, training_labels, beta, b, lam)\n",
    "\n",
    "    return loss, new_beta, new_b\n",
    "\n",
    "\n",
    "def predict_svm(beta, b, test_features):\n",
    "    predicted_labels = []\n",
    "    for i in range(len(test_features)):\n",
    "        #look at the sign after applying the prediction line\n",
    "        predicted_label = np.sign(test_features[i].dot(beta) + b)\n",
    "        predicted_labels.append(predicted_label[0])\n",
    "    return predicted_labels\n",
    "\n",
    "def verify_prediction_svm(predicted_labels, test_labels):\n",
    "    correct_labels = 0\n",
    "    total_labels = len(predicted_labels)\n",
    "    #Compare predicted labels to actual labels\n",
    "    for i in range(total_labels):\n",
    "        if (predicted_labels[i] == test_labels[i]):\n",
    "            correct_labels += 1\n",
    "        elif (predicted_labels[i] == test_labels[i]):\n",
    "            correct_labels += 1\n",
    "    return correct_labels/total_labels\n",
    "        \n",
    "\n",
    "def train_and_verify_svm(training_features, training_labels, lam, steps, learning_rate):\n",
    "    # Initialize required variables\n",
    "    number_of_features = len(training_features[0])\n",
    "    beta = np.random.normal(size=(number_of_features, 1))\n",
    "    b = 0\n",
    "    losses = []\n",
    "    training_errors = []\n",
    "    training_rate_adjusted = False\n",
    "\n",
    "    # train for steps steps\n",
    "    for i in range(steps):\n",
    "        loss, beta, b = fit_svm(training_features, training_labels, beta, b, lam, learning_rate)\n",
    "        losses.append(loss[0][0])\n",
    "\n",
    "        # After every step predict for the training set and look at accuracy\n",
    "        predicted_labels = predict_svm(beta, b, training_features)\n",
    "        training_error = verify_prediction_svm(predicted_labels, training_labels)\n",
    "        training_errors.append(training_error)\n",
    "\n",
    "        # If at least 2 steps have been run check if the training error changes in a meaningful way by checking if the error changed in the last two steps\n",
    "        if len(training_errors) > 1:\n",
    "            # If the error doesn't change change the learning rate once\n",
    "            if(training_errors[-1] == training_errors[-2] and not training_rate_adjusted):\n",
    "                training_rate_adjusted = True\n",
    "                learning_rate = learning_rate/10\n",
    "\n",
    "    return beta, b, losses, training_errors\n",
    "\n",
    "# Change the 3s to -1 and the 9s to 1 for the test and train set\n",
    "y_train[i==3] = -1\n",
    "y_train[i==9] = 1\n",
    "\n",
    "y_test[i==3] = -1\n",
    "y_test[i==9] = 1\n",
    "\n",
    "#initalize training parameters and start training\n",
    "lam = 1\n",
    "steps = 300\n",
    "learning_rate = 0.1\n",
    "\n",
    "beta, b, losses, training_errors = train_and_verify_svm(X_train, y_train, lam, steps, learning_rate)\n",
    "\n",
    "predicted_labels = predict_svm(beta, b, X_test)\n",
    "\n",
    "# Plot the loss\n",
    "fig, ax = plt.subplots()  \n",
    "ax.set_xlabel('steps')\n",
    "ax.set_ylabel('loss')\n",
    "ax.plot(losses) \n",
    "\n",
    "# Plot the training error\n",
    "fig, ax = plt.subplots()  \n",
    "ax.set_xlabel('steps')\n",
    "ax.set_ylabel('training error')\n",
    "ax.plot(training_errors, label='Training Error over time') \n",
    "\n",
    "# Setup the cross validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "test_errors = []\n",
    "sklearn_test_errors = []\n",
    "lambdas = [0.01, 0.1, 1, 10, 100]\n",
    "steps = 20\n",
    "\n",
    "# iterate through all lambdas and look which one gives the best result\n",
    "for lam in lambdas:\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        x_train_fold, x_test_fold = X_train[train_index], X_train[test_index]\n",
    "        y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n",
    "        beta, b, losses, training_errors = train_and_verify_svm(x_train_fold, y_train_fold, lam, steps, learning_rate)\n",
    "\n",
    "        predicted_labels = predict_svm(beta, b, x_test_fold)\n",
    "        test_error = np.mean(predicted_labels != y_test_fold)\n",
    "    test_errors.append(test_error)\n",
    "    \n",
    "print(\"This lambda performed best\" + str(lambdas[np.argmax(np.average(test_errors))]))\n",
    "        \n",
    "# Look at the test error of sthe sklearn implementation\n",
    "svm = LinearSVC()\n",
    "svm.fit(x_train_fold, y_train_fold)\n",
    "sklearn_predicted_labels = svm.predict(x_test_fold)\n",
    "sklearn_test_error = np.mean(sklearn_predicted_labels != y_test)\n",
    "\n",
    "print(\"Our svm test error with best lambda: \", np.mean(test_errors))\n",
    "print(\"Sklearn svm test error: \", np.mean(sklearn_test_errors))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
