{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">\n",
    "Comments for Exercise 01: <br/>\n",
    "Comment here\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">\n",
    "Comments for Exercise 02: <br/>\n",
    "Comment here\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">\n",
    "Comments for Exercise 03: <br/>\n",
    "Our forward pass does the same as the sample solution. The sample solution however works with copies of the input data to then make the calculations in place with array manipulation. <br/>\n",
    "Our backward propagation however is wrong. We do not need to update our parameters B and b in this layer and do not multiply the upstream gradient with the parameter B. The sample solution which applies the relu to the input is sufficient.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLULayer(object):\n",
    "    def forward(self, input):\n",
    "        # remember the input for later backpropagation\n",
    "        self.input = input\n",
    "        # return the ReLU of the input\n",
    "        relu = np.maximum(0, input)\n",
    "        return relu\n",
    "\n",
    "    def backward(self, upstream_gradient):\n",
    "        # compute the derivative of the weights from upstream_gradient and the stored input\n",
    "        self.grad_b = np.sum(upstream_gradient, axis=0)\n",
    "        self.grad_B = np.dot(self.input.T, upstream_gradient)\n",
    "        # compute the downstream gradient to be passed to the preceding layer\n",
    "        downstream_gradient = np.dot(upstream_gradient, self.B.T)\n",
    "        return downstream_gradient\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        pass # ReLU is parameter-free"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">\n",
    "Comments for Exercise 03: <br/>\n",
    "The forward pass is functionally the same as the sample solution, simply implementing the softmax function. <br/>\n",
    "The backward pass is also similar to the sample solution but written differently\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputLayer(object):\n",
    "    def __init__(self, n_classes):\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def forward(self, input):\n",
    "        # remember the input for later backpropagation\n",
    "        self.input = input\n",
    "        # return the softmax of the input        \n",
    "        if np.isnan(input).any():\n",
    "            input[np.isnan(input)] = 0.0\n",
    "        input_shifted = input - np.max(input, axis=1, keepdims=True)\n",
    "        exp_input = np.exp(input_shifted)\n",
    "        softmax = exp_input / np.sum(exp_input, axis=1, keepdims=True)\n",
    "        return softmax\n",
    "\n",
    "    def backward(self, predicted_posteriors, true_labels):\n",
    "        # return the loss derivative with respect to the stored inputs\n",
    "        # (use cross-entropy loss and the chain rule for softmax,\n",
    "        #  as derived in the lecture)\n",
    "        downstream_gradient = (predicted_posteriors - true_labels) / len(true_labels) # your code here\n",
    "        return downstream_gradient\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        pass # softmax is parameter-free"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">\n",
    "Comments for Exercise 03: <br/>\n",
    "The sample solution specifies a span the values can take, while we do not. This is likely a better approach as the initial guess will be better. <br/>\n",
    "Our forward pass is the same as the sample solution, with the only difference being added exception handling in our solution. <br/>\n",
    "The backwards propagation is identical to the sample solution\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(object):\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        self.n_inputs  = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        # randomly initialize weights and intercepts\n",
    "        self.B = np.random.normal(size=(self.n_inputs, self.n_outputs)) \n",
    "        self.b = np.random.normal(size=n_outputs) \n",
    "\n",
    "    def forward(self, input):\n",
    "        # remember the input for later backpropagation\n",
    "        global preactivations\n",
    "        self.input = input\n",
    "        # compute the scalar product of input and weights\n",
    "        # (these are the preactivations for the subsequent non-linear layer)\n",
    "        try:\n",
    "            preactivations = np.dot(input, self.B) + self.b\n",
    "        except:\n",
    "            print(\"Exception\")\n",
    "        return preactivations\n",
    "\n",
    "    def backward(self, upstream_gradient):\n",
    "        # compute the derivative of the weights from\n",
    "        # upstream_gradient and the stored input\n",
    "        # b is the first entry of Beta so it is updated by multiplying the first entry of the upstream gradient with it\n",
    "        # Beta is updated by multiplying it with the upstream gradient element wise\n",
    "        self.grad_b = np.sum(upstream_gradient, axis=0)\n",
    "        self.grad_B = np.dot(self.input.T, upstream_gradient)  # your code here\n",
    "        # compute the downstream gradient to be passed to the preceding layer\n",
    "        downstream_gradient = np.dot(upstream_gradient, self.B.T)\n",
    "        return downstream_gradient\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        # update the weights by batch gradient descent\n",
    "        self.B = self.B - learning_rate * self.grad_B\n",
    "        self.b = self.b - learning_rate * self.grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">\n",
    "Comments for Exercise 03: <br/>\n",
    "We essentially have same approach as the sample solution. We have however some inefficiencies and errors in our code. First of all we calculate the initial gradient twice. Once before the loop and once inside the loop. Second of all we skip the backward function of the relulayer by only looking at the output layer and the linearlayers. So we should adapt the iteration of the for loop to also include the relulayer and remove the duplicate calculations.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    def __init__(self, n_features, layer_sizes):\n",
    "        # constuct a multi-layer perceptron\n",
    "        # with ReLU activation in the hidden layers and softmax output\n",
    "        # (i.e. it predicts the posterior probability of a classification problem)\n",
    "        #\n",
    "        # n_features: number of inputs\n",
    "        # len(layer_size): number of layers\n",
    "        # layer_size[k]: number of neurons in layer k\n",
    "        # (specifically: layer_sizes[-1] is the number of classes)\n",
    "        self.n_layers = len(layer_sizes)\n",
    "        self.layers   = []\n",
    "\n",
    "        # create interior layers (linear + ReLU)\n",
    "        n_in = n_features\n",
    "        for n_out in layer_sizes[:-1]:\n",
    "            self.layers.append(LinearLayer(n_in, n_out))\n",
    "            self.layers.append(ReLULayer())\n",
    "            n_in = n_out\n",
    "\n",
    "        # create last linear layer + output layer\n",
    "        n_out = layer_sizes[-1]\n",
    "        self.layers.append(LinearLayer(n_in, n_out))\n",
    "        self.layers.append(OutputLayer(n_out))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X is a mini-batch of instances\n",
    "        batch_size = X.shape[0]\n",
    "        # flatten the other dimensions of X (in case instances are images)\n",
    "        X = X.reshape(batch_size, -1)\n",
    "\n",
    "        # compute the forward pass\n",
    "        # (implicitly stores internal activations for later backpropagation)\n",
    "        result = X\n",
    "        for layer in self.layers:\n",
    "            result = layer.forward(result)\n",
    "        return result\n",
    "\n",
    "    def backward(self, predicted_posteriors, true_classes):\n",
    "        # perform backpropagation w.r.t. the prediction for the latest mini-batch X\n",
    "        # Initialize the loss gradient by using the backward function in the output layer\n",
    "        batch_size = len(true_classes)\n",
    "        true_classes = true_classes.reshape((batch_size, 1))\n",
    "        downstream_gradient = (predicted_posteriors - true_classes) / batch_size\n",
    "        for layer in reversed(self.layers):\n",
    "            if isinstance(layer, OutputLayer):\n",
    "                downstream_gradient = layer.backward(predicted_posteriors, true_classes)\n",
    "            elif isinstance(layer, LinearLayer):\n",
    "                downstream_gradient = layer.backward(downstream_gradient)\n",
    "        return downstream_gradient\n",
    "\n",
    "    def update(self, X, Y, learning_rate):\n",
    "        posteriors = self.forward(X)\n",
    "        self.backward(posteriors, Y)\n",
    "        for layer in self.layers:\n",
    "            layer.update(learning_rate)\n",
    "\n",
    "    def train(self, x, y, n_epochs, batch_size, learning_rate):\n",
    "        N = len(x)\n",
    "        n_batches = N // batch_size\n",
    "        for i in range(n_epochs):\n",
    "            # print(\"Epoch\", i)\n",
    "            # reorder data for every epoch\n",
    "            # (i.e. sample mini-batches without replacement)\n",
    "            permutation = np.random.permutation(N)\n",
    "\n",
    "            for batch in range(n_batches):\n",
    "                # create mini-batch\n",
    "                start = batch * batch_size\n",
    "                x_batch = x[permutation[start:start+batch_size]]\n",
    "                y_batch = y[permutation[start:start+batch_size]]\n",
    "\n",
    "                # perform one forward and backward pass and update network parameters\n",
    "                self.update(x_batch, y_batch, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">\n",
    "Comments for Exercise 03: <br/>\n",
    "Our solution is nearly identical to the sample solution only doing the mean and sum simultaneously instead of after one another.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error rate: 0.83\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    # set training/test set size\n",
    "    N = 2000\n",
    "\n",
    "    # create training and test data\n",
    "    X_train, Y_train = datasets.make_moons(N, noise=0.05)\n",
    "    X_test,  Y_test  = datasets.make_moons(N, noise=0.05)\n",
    "    n_features = 2\n",
    "    n_classes  = 2\n",
    "\n",
    "    # standardize features to be in [-1, 1]\n",
    "    offset  = X_train.min(axis=0)\n",
    "    scaling = X_train.max(axis=0) - offset\n",
    "    X_train = ((X_train - offset) / scaling - 0.5) * 2.0\n",
    "    X_test  = ((X_test  - offset) / scaling - 0.5) * 2.0\n",
    "\n",
    "    # set hyperparameters (play with these!)\n",
    "    layer_sizes = [5, 5, n_classes]\n",
    "    n_epochs = 5\n",
    "    batch_size = 200\n",
    "    learning_rate = 0.05\n",
    "\n",
    "    # create network\n",
    "    network = MLP(n_features, layer_sizes)\n",
    "\n",
    "    # train\n",
    "    network.train(X_train, Y_train, n_epochs, batch_size, learning_rate)\n",
    "\n",
    "    # test\n",
    "    predicted_posteriors = network.forward(X_test)\n",
    "    # determine class predictions from posteriors by winner-takes-all rule\n",
    "    predicted_classes = np.argmax(predicted_posteriors, axis=1)\n",
    "    # compute and output the error rate of predicted_classes\n",
    "    error_rate = np.mean(predicted_classes != Y_test.reshape(-1))\n",
    "    print(\"error rate:\", error_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">\n",
    "Comments for Exercise 03: <br/>\n",
    "The last part of the exercise of trying the process for different variations in the variables was not present in the sample solution.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Error for various layers:\n",
      "-----------------------------\n",
      "Layer Sizes:  [2, 2, 2]\n",
      "Validation Error:  0.866\n",
      "-----------------------------\n",
      "Layer Sizes:  [3, 3, 2]\n",
      "Validation Error:  0.5\n",
      "-----------------------------\n",
      "Layer Sizes:  [5, 5, 2]\n",
      "Validation Error:  0.121\n",
      "-----------------------------\n",
      "Layer Sizes:  [30, 30, 2]\n",
      "Validation Error:  0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding\\MSE-Exercises\\.conda\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:88: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    }
   ],
   "source": [
    "def calculate_validation_error(network, X_val, Y_val):\n",
    "    predicted_posteriors = network.forward(X_val)\n",
    "    predicted_classes = np.argmax(predicted_posteriors, axis=1)\n",
    "    error_rate = np.mean(predicted_classes != Y_val)\n",
    "    return error_rate\n",
    "\n",
    "layer_sizes_list = [[2, 2, n_classes], [3, 3, n_classes], [5, 5, n_classes], [30, 30, n_classes]]\n",
    "print(\"\\nValidation Error for various layers:\")\n",
    "\n",
    "# Train and compare the networks\n",
    "for layer_sizes in layer_sizes_list:\n",
    "    network = MLP(n_features, layer_sizes)\n",
    "    network.train(X_train, Y_train, n_epochs=5, batch_size=200, learning_rate=0.05)\n",
    "    validation_error = calculate_validation_error(network, X_test, Y_test)\n",
    "    print(\"-----------------------------\")\n",
    "    print(\"Layer Sizes: \", layer_sizes)\n",
    "    print(\"Validation Error: \", validation_error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
